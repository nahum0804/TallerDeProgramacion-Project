{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "-7v5pJg20W3P",
        "Horrxr0r0rGb",
        "Iqvku4LF07HE",
        "zthi61JB1MUn"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nahum0804/TallerDeProgramacion-Project/blob/main/Reconocer_rostros_Dylan_y_Nahum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiHwhkjUJDi3"
      },
      "source": [
        "# Instituto Tecnológico de Costa Rica | TEC\n",
        "# Carrera de Ingeniería en Computación \n",
        "# I Semestre de 2023\n",
        "# Proyecto Programado (Fase I – Entrevistador asistido por IA) \n",
        "# Dr. Abel Méndez Porras\n",
        "# amendez@itcr.ac.cr\n",
        "# Porcentaje: 20%"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrevistador asistido por IA\n",
        "\n"
      ],
      "metadata": {
        "id": "bLtEFwW88r6C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El reconocimiento facial y la detección de emociones son tecnologías que se han vuelto cada vez más populares en los últimos años. Estas tecnologías utilizan algoritmos de inteligencia artificial para analizar imágenes y detectar patrones faciales que permiten identificar a las personas y sus emociones.\n",
        "En cuanto a las oportunidades, el reconocimiento facial puede tener aplicaciones en la seguridad, el marketing y la atención al cliente. Por ejemplo, en la seguridad se pueden utilizar sistemas de reconocimiento facial para identificar a personas en lugares públicos o para controlar el acceso a edificios y sistemas de seguridad. En el marketing, se pueden utilizar para personalizar la publicidad en función de la edad, el género y otros datos demográficos. Y en la atención al cliente, se pueden utilizar para detectar la satisfacción del cliente en tiempo real y responder adecuadamente.\n",
        "\n",
        "En esta primera etapa del proyecto del curso de Taller de Programación estaremos utilizando los resultados de consultas a modelos de Deep Learning para aplicar los conocimientos adquiridos en clase. "
      ],
      "metadata": {
        "id": "nTAGLkSC_tPv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objetivo\n",
        "Desarrollar una aplicación de complejidad baja que permita poner en práctica los conocimientos adquiridos en los cursos de Introducción a la Programación y Taller de Programación.\n",
        "## Objetivos Específicos\n",
        "*   Identificar los requerimientos para resolver un problema específico desde la perspectiva de resolución de problemas, técnicas con listas y diccionarios.\n",
        "*   Diseñar una aplicación para un problema específico desde la perspectiva de resolución de problemas, técnicas con listas y diccionarios.\n",
        "*.  Desarrollar una aplicación para un problema específico desde la perspectiva de resolución de problemas, técnicas con listas y diccionarios."
      ],
      "metadata": {
        "id": "BluCjMmS3ySE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configuración previa "
      ],
      "metadata": {
        "id": "CsQAX0tRIPs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cargar Google Drive"
      ],
      "metadata": {
        "id": "BKtEWO28-Psm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRct28jc9-Zk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6bce403-259a-4cc7-aecf-a9e993b518ad"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Leer cámara\n",
        "Tomado de https://colab.research.google.com/drive/1QnC7lV7oVFk5OZCm75fqbLAfD9qBy9bw?usp=sharing#scrollTo=ilLkpcKanPRb\n",
        "\n"
      ],
      "metadata": {
        "id": "qcsaTisaz7gf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importar dependencies"
      ],
      "metadata": {
        "id": "-7v5pJg20W3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import dependencies\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time"
      ],
      "metadata": {
        "id": "sFbwWU1E0RuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convertir el objeto JavaScript un objeto OpenCV image"
      ],
      "metadata": {
        "id": "Horrxr0r0rGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ],
      "metadata": {
        "id": "zgaKzXQP0no3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Crear nuestro live video stream"
      ],
      "metadata": {
        "id": "Iqvku4LF07HE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "metadata": {
        "id": "ud8m7Jqf010R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Iniciar streaming video"
      ],
      "metadata": {
        "id": "zthi61JB1MUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image, ImageDraw, ImageFont"
      ],
      "metadata": {
        "id": "ZrXBaQowmi25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.models import model_from_json"
      ],
      "metadata": {
        "id": "79ydx6jivUDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cargar modelos de Deep Learning entrenados"
      ],
      "metadata": {
        "id": "-PYe1k24Alu1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Para detectar los rostros se utiliza el modelo de Face Haar Cascade.\n",
        "*   Para detectar las emociones se utiliza el modelo de VGG.\n",
        "*   También se utiliza un algoritmo adicional para pasar la información del algortimo de VGG a un formato JSON.\n",
        "\n",
        "Estos modelos y algoritmos están disponibles en el siguiente enlace [Descargar modelos y algoritmos](https://drive.google.com/drive/folders/1KctA_s25bqqMOUlQuy0lwTm0HkBGVycF?usp=sharing). Usted los debe colocar en su Google Drive y modificar la ruta de acceso. \n",
        "\n"
      ],
      "metadata": {
        "id": "SFuMVQbFBG_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargar modelos de Deep Learning entrenados\n",
        "face_haar_cascade = cv2.CascadeClassifier('/content/drive/MyDrive/projectFiles/haarcascade_frontalface_alt.xml') \n",
        "model = model_from_json(open(\"/content/drive/MyDrive/projectFiles/model.json\", \"r\").read()) \n",
        "model.load_weights('/content/drive/MyDrive/projectFiles/model.h5') \n"
      ],
      "metadata": {
        "id": "e1w94AggiPn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crear interfaz\n",
        "https://www.youtube.com/watch?v=oIVmV41uyK8"
      ],
      "metadata": {
        "id": "K_5yU73nzHJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Entrevista personal"
      ],
      "metadata": {
        "id": "aV92oOkb4Qcc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recolectar información personal"
      ],
      "metadata": {
        "id": "HMLUntgiGJsf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Información requerida al entrevistar una persona (Toda esta información debe ser almacenada en un diccionario. Cada diccionario de cada entrevista debe ser colocado en una lista. Se maneja una lista principal donde se tiene todas las entrevistas de las personas).\n",
        "*   Identificación  \n",
        "*   Nombre \n",
        "*   Fecha de nacimiento\n",
        "*   Nacionalidad\n",
        "*   Números de teléfono y celular (un diccionario)\n",
        "*   Emails (una lista con uno o varios emails)\n",
        "*   Dirección\n",
        "*   Estado civil\n",
        "*   Cantidad de hijos indicando el género, la fecha de nacimiento(un diccionario) \n",
        "*   Entrevistas (cada persona puede ser entrevistada cero o más veces). Las entrevistas se realizan en tiempo real utilizando una camára integrada a la computadora o externa (el profesor facilita el código para activar la camára, capturar el video y capturar los frames cada cierto tiempo). Se debe crear una lista con sublistas donde cada sublista tiene las imagenes capturas y las emociones reportadas durante la entrevista. Además, debe guardar coordenadas donde se encuentra el rostro en la imagen. Para cada entrevista, el entrevistado debe contar historias cortas sobre los siguientes 4 tópicos:\n",
        "\n",
        "\n",
        "  >*   Una experiencia de sorpresa o asombro que ha tenido que vivir.\n",
        "  >*   Una experiencia de tristeza que ha tenido que vivir.\n",
        "  >*   Una experiencia de enojo o frustración que ha tenido que vivir.\n",
        "  >*   Una experiencia de felicidad que ha tenido que vivir.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fUUU_SL55RSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Global variables\n",
        "userList = []\n",
        "dictionaryList = []\n",
        "\n",
        "def createUserAndAttributesFace(faces_detected, predictions, max_index, predictionsP, frame_dic, emotion_detection, image_path):\n",
        "\n",
        "  \"\"\" Create user dictionary \"\"\"\n",
        "\n",
        "  while True:\n",
        "    name = input(\"Enter your name: \")\n",
        "\n",
        "    dictionary = dict()\n",
        "    dictionary[name] = dict()\n",
        "    dictionary[name][\"ID\"] = input(\"Enter your ID: \")\n",
        "    dictionary[name][\"BirthDay\"] = input(\"Enter your birthday (Format: DD-MM-YYYY): \")\n",
        "    dictionary[name][\"Nacionality\"] = input(\"Enter your nacionality: \")\n",
        "    dictionary[name][\"Cellphone\"] = dict()\n",
        "   \n",
        "    i = 1\n",
        "    while True:\n",
        "      cellphone = input(\"Cellphone: \")\n",
        "      dictionary[name][\"Cellphone\"][f\"Cellphone-{i}\"] = cellphone\n",
        "      i += 1\n",
        "      option = input(\"You want add another cellphone number? Y/N: \")\n",
        "      if option == \"Y\":\n",
        "       continue\n",
        "      elif option == \"N\":\n",
        "        break\n",
        "      else:\n",
        "        print(\"Invalid option - (You most enter the options in capital letters!)\")\n",
        "        break\n",
        "\n",
        "    dictionary[name][\"email\"] = []\n",
        "    while True:\n",
        "      email = input(\"Email: \")\n",
        "      dictionary[name][\"email\"].append(email)\n",
        "      option = input(\"You want add another email? Y/N: \")\n",
        "      if option == \"Y\":\n",
        "       continue\n",
        "      elif option == \"N\":\n",
        "        break\n",
        "      else:\n",
        "        print(\"Invalid option - (You most enter the options in capital letters!)\")\n",
        "        break\n",
        "\n",
        "    dictionary[name][\"location\"] = input(\"Enter your location: \")\n",
        "    dictionary[name][\"civilStatus\"] = input(\"Enter your civil status: \")\n",
        "    dictionary[name][\"children\"] = dict()\n",
        "    children = input(\"Do you have children? Y/N: \")\n",
        "    if children == \"Y\":\n",
        "      valid = True\n",
        "      while valid:\n",
        "        nameChild = input(\"Name: \")\n",
        "        gender = input(\"Gender: \")\n",
        "        birthDay = input(\"Birth day (Format: DD-MM-YYYY): \")\n",
        "\n",
        "        dictionary[name][\"children\"][nameChild] = dict()\n",
        "        dictionary[name][\"children\"][nameChild][\"gender\"] = gender\n",
        "        dictionary[name][\"children\"][nameChild][\"birthDay\"] = birthDay\n",
        "\n",
        "        option = input(\"You want add another child? Y/N: \")\n",
        "        if option == \"Y\":\n",
        "          continue\n",
        "        elif option == \"N\":\n",
        "          valid = False\n",
        "        else:\n",
        "          print(\"Invalid option - (You most enter the options in capital letters!)\")\n",
        "          valid = False\n",
        "    elif children == \"N\":\n",
        "      continue\n",
        "    else:\n",
        "      print(\"Invalit option\")\n",
        "\n",
        "\n",
        "    dictionary[name][\"imagesCaptured\"] = []  #List for images captured and emotions gained\n",
        "\n",
        "    dictionary[name][\"coordinates\"] = [] \n",
        "    \n",
        "      \n",
        "    print(\"Ubicación del rostro:\", faces_detected)\n",
        "    print(\"Predicciones: \", predictions)\n",
        "    print(\"Emocion mas significativa: \",max_index, predictions[0])\n",
        "    print(\"Predicciones: \", frame_dic)\n",
        "    print(\"Emocion: \",emotion_prediction)\n",
        "    print(\"Location: \", image_path)\n",
        "\n",
        "\n",
        "    dictionaryList.append(dictionary)\n",
        "    userList.append(dictionaryList)\n",
        "    print(userList)\n",
        "\n",
        "    select = input(\"You want continue adding users? Y/N: \")\n",
        "    if select == \"Y\":\n",
        "      continue\n",
        "    elif select == \"N\":\n",
        "      break\n",
        "    else:\n",
        "      print(\"Invalid option - (You most enter the options in capital letters!)\")\n",
        "      break\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zfsivGpD4Mms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recolectar información emociones\n",
        "Este código se encarga de ejecutar la captura de video de la cámara, detectar los rostros en las imágenes y reconocer las emociones presentes en los rostros. Cuando se hace la entrevista es en este código donde usted de obtener la información para crear las listas con la información solicitada."
      ],
      "metadata": {
        "id": "7mXDD7r7GSec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.ops.math_ops import sign\n",
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0 \n",
        "j = 5\n",
        "while True:\n",
        "  js_reply = video_frame(label_html, bbox)\n",
        "  if not js_reply:\n",
        "    break\n",
        "\n",
        "  # convert JS response to OpenCV Image\n",
        "  img = js_to_image(js_reply[\"img\"])\n",
        "  img_copy = img\n",
        "\n",
        "  # create transparent overlay for bounding box\n",
        "  bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "\n",
        "\n",
        "  if count == j: #El tiempo para capturar el frame y procesarlo\n",
        "    #Este sería el path de la imagen guardada\n",
        "    image_path ='/content/drive/MyDrive/projectFiles/imagesGained/' + 'nombre' + str(j) +  '.jpg'\n",
        "    #se guarda el fotograma con el \"nombre\" más el número del contador i\n",
        "    cv2.imwrite(image_path , img) \n",
        "    \n",
        "    #Detectar los rostros usando Face Haar Cascade\n",
        "    gray_image = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    #En faces_deteted se encuentran los rotros detectados\n",
        "    faces_detected = face_haar_cascade.detectMultiScale(gray_image,1.32,5)\n",
        "    #Imprimir ubicación de los rostros en la imagen\n",
        "    print(\"Ubicación del rostro:\", faces_detected)\n",
        "    im2Display = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    imTemp = im2Display.copy()\n",
        "\n",
        "    #Hacer un rectángulo en cada rostro detectado en la imagen\n",
        "    for (x,y,w,h) in faces_detected:\n",
        "        cv2.rectangle(img,(x,y), (x+w,y+h), (255,0,0), thickness=7)\n",
        "\n",
        "        roi_gray=gray_image[y:y+w,x:x+h]\n",
        "        roi_gray=cv2.resize(roi_gray,(48,48))\n",
        "\n",
        "        #Processes the image and adjust it to pass it to the model\n",
        "        image_pixels = tf.keras.preprocessing.image.img_to_array(roi_gray)\n",
        "       \n",
        "        image_pixels = np.expand_dims(image_pixels, axis = 0)\n",
        "        image_pixels /= 255\n",
        "\n",
        "        #Obtener las emociones para cada rostro detectado en la imagen\n",
        "        #Get the prediction of the model\n",
        "        predictions = model.predict(image_pixels)\n",
        "        #Mostrar todas las emociones detectadas\n",
        "        print(\"Predicciones: \", predictions)\n",
        "        #Mostrar la emoción más significativa (con el valor más alto)\n",
        "        max_index = np.argmax(predictions[0])\n",
        "        print(max_index, predictions[0])\n",
        "        #El orden en que vienen las emociones\n",
        "        emotion_detection = ('angry', 'disgust', 'fear', 'happyness', 'sadness', 'surprise', 'neutral')\n",
        "\n",
        "        emotion_prediction = emotion_detection[max_index]\n",
        "        #Crear un diccionario para almacenar la emoción y el valor para la emoción\n",
        "        frame_dic = dict(\n",
        "            angry = predictions[0][0],\n",
        "            disgust = predictions[0][1],\n",
        "            fear = predictions[0][2],            \n",
        "            happiness = predictions[0][3],\n",
        "            sadness = predictions[0][4],\n",
        "            surprise = predictions[0][5],\n",
        "            neutral = predictions[0][6]) \n",
        "        print(\"Predicciones: \", frame_dic)\n",
        "        print(emotion_prediction)\n",
        "        \n",
        "        #Mostrar en la imagen un texto de la emoción más significativa\n",
        "        ubicacion = (x,y)\n",
        "        font = cv2.FONT_HERSHEY_TRIPLEX\n",
        "        tamañoLetra = 2\n",
        "        colorLetra = (221,82,196)\n",
        "        grosorLetra = 7\n",
        "        #Escribir texto con la emoción más significativa\n",
        "        cv2.putText(imTemp, emotion_prediction, ubicacion, font, tamañoLetra, colorLetra, grosorLetra)\n",
        "        pt1 = (x, y)\n",
        "        pt2 = (x+w, y+h)\n",
        "        color = (23,200,54)\n",
        "        thickness = 10\n",
        "        cv2.rectangle(imTemp, pt1, pt2, color, thickness)\n",
        "\n",
        "    #Mostrar la imagen con los cuadros en los rostros y el texto de la emoción más significativa\n",
        "    plt.imshow(imTemp)\n",
        "    plt.show()\n",
        "\n",
        "    j +=5\n",
        "  count +=1\n",
        "\n",
        "  #----------------------Recolect Data---------------------------\n",
        "  createUserAndAttributesFace(faces_detected, predictions, max_index, predictions[0], frame_dic, emotion_detection, image_path) #Testing send gained data\n"
      ],
      "metadata": {
        "id": "KOwjK9cMGRzv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3a200384-e7b3-4be6-d45a-d78f44351c8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "    \n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "    \n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "    \n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "    \n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "           \n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "      \n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML = \n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "      \n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 640; //video.videoWidth;\n",
              "      captureCanvas.height = 480; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "      return stream;\n",
              "    }\n",
              "    async function stream_frame(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "      \n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "            \n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      \n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "      \n",
              "      return {'create': preShow - preCreate, \n",
              "              'show': preCapture - preShow, \n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your name: Nahum\n",
            "Enter your ID: 208560927\n",
            "Enter your birthday (Format: DD-MM-YYYY): 30-08-2004\n",
            "Enter your nacionality: Costa Rican\n",
            "Cellphone: 6070-6962\n",
            "You want add another cellphone number? Y/N: N\n",
            "Email: n4hummuro227@gmail.com\n",
            "You want add another email? Y/N: N\n",
            "Enter your location: San Carlos, Alajuela\n",
            "Enter your civil status: Single\n",
            "Do you have children? Y/N: N\n",
            "Enter your name: \n",
            "Enter your ID: \n",
            "Enter your birthday (Format: DD-MM-YYYY): \n",
            "Enter your nacionality: \n",
            "Cellphone: \n",
            "You want add another cellphone number? Y/N: \n",
            "Invalid option - (You most enter the options in capital letters!)\n",
            "Email: \n",
            "You want add another email? Y/N: \n",
            "Invalid option - (You most enter the options in capital letters!)\n",
            "Enter your location: \n",
            "Enter your civil status: \n",
            "Do you have children? Y/N: \n",
            "Invalit option\n",
            "Ubicación del rostro: [[221 183 210 210]]\n",
            "Predicciones:  [[3.7497102e-04 2.5371564e-06 2.9345797e-04 9.7075731e-01 5.8985961e-04\n",
            "  5.8405485e-04 2.7397845e-02]]\n",
            "Emocion mas significativa:  3 [3.7497102e-04 2.5371564e-06 2.9345797e-04 9.7075731e-01 5.8985961e-04\n",
            " 5.8405485e-04 2.7397845e-02]\n",
            "Predicciones:  {'angry': 0.00037497102, 'disgust': 2.5371564e-06, 'fear': 0.00029345797, 'happiness': 0.9707573, 'sadness': 0.0005898596, 'surprise': 0.00058405485, 'neutral': 0.027397845}\n",
            "Emocion:  happyness\n",
            "Location:  /content/drive/MyDrive/projectFiles/imagesGained/nombre5.jpg\n",
            "[[{'': {'ID': '', 'BirthDay': '', 'Nacionality': '', 'Cellphone': {'Cellphone-1': ''}, 'email': [''], 'location': '', 'civilStatus': '', 'children': {}, 'imagesCaptured': [], 'coordinates': []}}]]\n",
            "You want continue adding users? Y/N: N\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-088c8499d449>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;31m#----------------------Recolect Data---------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   \u001b[0mcreateUserAndAttributesFace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfaces_detected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_dic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memotion_detection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-39-e5bf4d0afc04>\u001b[0m in \u001b[0;36mcreateUserAndAttributesFace\u001b[0;34m(faces_detected, predictions, max_index, predictionsP, frame_dic, emotion_detection, image_path)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your name: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Consultas\n",
        "\n",
        "\n",
        "1.   Mostrar la información de todas las personas entrevistas (la información debe ser mostrada siendo saca de los diccionarios o listas, no se debe hacer simplemente la impresión en pantalla de los diccionarios o las listas). Además, de mostrar toda la información recolectada de la persona, se debe mostrar todas las imágenes que se recolectaron durante las entrevistas, las emociones reconocidas para cada imagen y las coordenadas donde se encuentra los rostros.\n",
        "\n",
        "2.   Mostrar la información de todas las personas entrevistas ordenada ascendentemente por edad. El método de ordenamiento a utilizar es el método por inserción (la información debe ser mostrada siendo saca de los diccionarios o listas, no se debe hacer simplemente la impresión en pantalla de los diccionarios o las listas). Además, de mostrar toda la información recolectada de la persona, se debe mostrar todas las imágenes que se recolectaron durante las entrevistas, las emociones reconocidas para cada imagen y las coordenadas donde se encuentra los rostros.\n",
        "\n",
        "3.   Mostrar la información de todas las personas entrevistas ordenada descendente mente por la cantidad de hijos. El método de ordenamiento a utilizar es el método por de la bubuja (la información debe ser mostrada siendo saca de los diccionarios o listas, no se debe hacer simplemente la impresión en pantalla de los diccionarios o las listas). Además, de mostrar toda la información recolectada de la persona, se debe mostrar todas las imágenes que se recolectaron durante las entrevistas, las emociones reconocidas para cada imagen y las coordenadas donde se encuentra los rostros.\n",
        "\n",
        "4. Mostrar la información de todas las personas entrevistas ordenada descendentemente por la cantidad de hijos. El método de ordenamiento a utilizar es el método por de la quicksort (la información debe ser mostrada siendo saca de los diccionarios o listas, no se debe hacer simplemente la impresión en pantalla de los diccionarios o las listas). Además, de mostrar toda la información recolectada de la persona, se debe mostrar todas las imágenes que se recolectaron durante las entrevistas, las emociones reconocidas para cada imagen y las coordenadas donde se encuentra los rostros. También, imprimir una lista con sublistas donde la primera sublista tiene la información de las personas que tienen únicamente hijos de género femenino, la segunda sublista tiene información de personas que tienen únicamente hijos de género masculino y la última sublista tiene información de personas que tienen hijos tanto de género femenino como másculino. \n",
        "\n",
        "5. Obtener para cada persona entrevistada las emociones sin ordenar y guardar en una lista (una lista por entrevista) con sublistas la siguiente información: en la primera sublista colocar la identificación, el nombre, el género y la edad de la persona entrevistada, en las siguientes sublistas colocar las emociones expresadas por la persona. Por ejemplo, si la persona apareció en 10 fotogramas entonces se deben crear 10 sublistas, cada sublista pertenece a las emociones emitidas por el candidato en un fotograma. \n",
        "\n",
        "6. Una vez creada esta listas (una por entrevista) con sublistas del punto anterior, se debe ordenar las sublistas de forma ascendente por la emoción felicidad (happiness) utilizando el técnica de ordenamiento Quicksort.\n",
        "\n",
        "7. Para cada persona entrevistada muestre un gráfico para comparar las emociones reportadas durante la entrevista.\n",
        "\n",
        "8. Incluya en el documento escrito la entrevista a dos personas. De aportar toda la información recolectada, las imágenes recolectadas en la entrevista con un cuadro en los rostros presentes. Además, debe mostrar la información de las emociones reportadas. Por último, incluya un análisis de la importancia de este tipo de algoritmos y de la importancia de la privacidad de la información en este tipo de sistemas.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VaTzgLRF8rdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aspectos administrativos"
      ],
      "metadata": {
        "id": "VYESKZg_IyIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "* La tarea será desarrollada en equipos de trabajo de máximo 2 personas.\n",
        "* Cualquier acción de plagio será sancionada con una nota de 0 y la carta al expediente respectiva para todos los\n",
        "implicados.\n",
        "* La fecha de entrega será la semana 11, el día de clases, antes de la hora de clases.\n",
        "* Para llevar el control de versiones del proyecto se debe crear un repositorio en GitLab y agregar al profesor (agregarlo\n",
        "a la cuenta amendez.tec@gmail.com).\n",
        "* Para llevar el control del las tareas a realizar por participante en el proyecto debe utilizar la herramienta\n",
        "https://trello.com/es y compartir el link en el mismo Google Doc en que se comparte el link de la plantilla de Overleaf. Pueden crear tab de todas las tareas o requerimientos, un tab para las tareas en proceso y un tab para las tareas hechas.\n",
        "* La codificación y documentación interna deberá ser desarrollada en idioma inglés utilizando el estándar docString de Python.\n",
        "*La documentación externa debe estar redactada en idioma inglés utilizando la herramienta Overleaf (utilizar la plantilla facilitada por el profesor). La documentación es evaluada en el curso de Introducción a la programación.\n",
        "* Documentación del Código utilizando Docstring Conventions \t\n",
        "* Introducción a Python la Guía de Estilo de Código en Python\n",
        "* La entrega será mediante la plataforma TecDigital en el espacio de Proyecto – Etapa 1. Lo que debe subir es un archivo de tipo notebook cuya extensión es (.ipynb). Además, en este archivo deben venir los enlaces a las imagenes utilizados para la tarea.\n",
        "* Se recomienda que se empiece a trabajar lo antes posible.\n",
        "\n"
      ],
      "metadata": {
        "id": "fuFcEU_WI2_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# \"La programación es una habilidad que puedes cultivar en ti mismo y usar para cambiar el mundo\". - Barack Obama"
      ],
      "metadata": {
        "id": "7uwbUKqmKhSC"
      }
    }
  ]
}